{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analítica Avanzada de Datos.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de clasificación\n",
    "En el último notebook ***02.Clasificacion_Binario*** ajustamos un clasificador binario para predecir si los pacientes eran diabéticos o no. Utilizamos el *accuracy* como medida del rendimiento del modelo, pero este no lo es todo. En este notebook, veremos alternativas de métricas que pueden ser mucho más útiles en el aprendizaje automático.\n",
    "\n",
    "## Métricas alternativas para clasificadores binarios\n",
    "El *accuracy* parece una métrica buena para evaluar (y hasta cierto punto lo es), pero hay que tener cuidado con sacar demasiadas conclusiones del *accuracy* de un clasificador. Recuerda que es simplemente una medida de cuántos casos se predijeron correctamente. Supongamos que sólo el 3% de la población es diabética. Se podría crear un clasificador que siempre predijera 0, y tendría una precisión del 97%, ¡pero no sería muy útil para identificar a los pacientes con diabetes!\n",
    "\n",
    "Afortunadamente, existen otras métricas que revelan algo más sobre el rendimiento de nuestro modelo. Scikit-Learn incluye la capacidad de crear un informe de clasificación que proporciona más información que la precisión bruta por sí sola.\n",
    "\n",
    "Para empezar, ejecuta la siguiente celda para cargar nuestros datos y entrenar nuestro modelo como la última vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Cargar el dataset de entrenamiento\n",
    "diabetes = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Separar las caracteristicas y clases\n",
    "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
    "label = 'Diabetic'\n",
    "X, y = diabetes[features].values, diabetes[label].values\n",
    "\n",
    "\n",
    "# Separamos el dataset 70%-30% en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "print ('Training cases: %d\\nTest cases: %d' % (X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "# Entrenamos el modelo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fijar la tasa de regularización\n",
    "reg = 0.01\n",
    "\n",
    "# entrenar un modelo de regresión logística en el conjunto de entrenamiento\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print('Predicted labels: ', predictions)\n",
    "print('Actual labels:    ' ,y_test)\n",
    "\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los lugares más sencillos para empezar es un informe de clasificación. Ejecuta la siguiente celda para ver una serie de formas alternativas de evaluar nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El informe de clasificación incluye las siguientes métricas para cada clase (0 y 1)\n",
    "\n",
    "> nota que la fila del encabezado puede no coincidir con los valores!\n",
    "\n",
    "* *Precision*: ¿Qué proporción de las predicciones realizadas por el modelo para esta clase fueron correctas?\n",
    "* *Recall*: De todos los casos de esta clase en el conjunto de datos de prueba, ¿cuántos identificó el modelo?\n",
    "* *F1-Score*: Una métrica media que tiene en cuenta tanto la precisión como la recuperación.\n",
    "* *Support*: ¿Cuántos casos de esta clase hay en el conjunto de datos de prueba?\n",
    "\n",
    "El informe de clasificación también incluye promedios para estas métricas, incluida una media ponderada que tiene en cuenta el desequilibrio en el número de casos de cada clase.\n",
    "\n",
    "Dado que se trata de un problema de *clasificación binaria*, la clase ***1*** se considera *positiva* y su precisión y recuperación son especialmente interesantes, ya que responden a las preguntas:\n",
    "\n",
    "- De todos los pacientes que el modelo predijo que eran diabéticos, ¿cuántos son realmente diabéticos?\n",
    "- De todos los pacientes que son realmente diabéticos, ¿cuántos identificó el modelo?\n",
    "\n",
    "Puede obtener estos valores por sí mismos utilizando las métricas **precision_score** y **recall_score** en scikit-learn (que por defecto asumen un modelo de clasificación binario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Overall Precision:\",precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\",recall_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las métricas de precisión y recall se derivan de cuatro posibles resultados de predicción:\n",
    "\n",
    "* *True Positives*: La clase predicha y la clase real son ambas 1.\n",
    "* *False Positives*: La clase predicha es 1, pero la clase real es 0.\n",
    "* *False Negatives*: La clase predicha es 0, pero la clase real es 1.\n",
    "* *True Negatives*: La clase predicha y la clase real son ambas 0.\n",
    "\n",
    "Estas métricas se tabulan generalmente para el conjunto de pruebas y se muestran juntas como una *matriz de confusión*, que tiene la siguiente forma:\n",
    "\n",
    "<table style=\"border: 1px solid black;\">\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FP</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TP</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Observa que las predicciones correctas (*verdaderas*) forman una línea diagonal desde arriba a la izquierda hasta abajo a la derecha: estas cifras deberían ser significativamente más altas que las predicciones *falsas* si el modelo es bueno.\n",
    "\n",
    "En Python, puede utilizar la función **sklearn.metrics.confusion_matrix** para encontrar estos valores para un clasificador entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Imprime la matriz de confusión\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora, hemos considerado las predicciones del modelo como clases de 1 o 0 clases. Peor los algoritmos estadísticos de aprendizaje automático, como la regresión logística, se basan en la probabilidad; por tanto, lo que realmente predice un clasificador binario es la probabilidad de que la clase sea verdadera (**P(y)**)  y la probabilidad de que la clase sea falsa (1 - **P(y)**). Se utiliza un valor umbral de 0.5 para decidir si la clase predicha es un 1 (*P(y) > 0.5*)  o un (*P(y) <= 0.5*). Puedes utilizar el método **predict_proba** para ver los pares de probabilidades para cada caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_scores = model.predict_proba(X_test)\n",
    "print(y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La decisión de puntuar una predicción como 1 o como 0 depende del umbral con el que se comparan las probabilidades predichas. Si cambiáramos el umbral, las predicciones se verían afectadas y, por tanto, cambiarían las métricas de la matriz de confusión. \n",
    "\n",
    "Una forma habitual de evaluar un clasificador es examinar la tasa de *true positive* (que es otro nombre para recall) y la tasa de *false positive* para un rango de posibles umbrales. A continuación, estas tasas se representan gráficamente frente a todos los umbrales posibles para formar una gráfica conocida como *received operator characteristic (ROC) chart*, como éste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# calcular la curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# dibujar la curva ROC\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Trazar la línea diagonal del 50%\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Representar gráficamente el FPR y el TPR obtenidos por nuestro modelo\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *ROC chart* muestra la curva de las tasas de verdaderos y falsos positivos para diferentes valores de umbral entre 0 y 1. Un clasificador perfecto tendría una curva que va recta hacia arriba por el lado izquierdo y recta por la parte superior. La línea diagonal que atraviesa el gráfico representa la probabilidad de predecir correctamente con una predicción aleatoria del 50/50, por lo que es obvio que se desea que la curva sea más alta que eso (¡o el modelo no será mejor que simplemente adivinar!).\n",
    "\n",
    "El área bajo la curva (AUC) es un valor entre 0 y 1 que cuantifica el rendimiento global del modelo. Cuanto más se acerque a 1 este valor, mejor será el modelo. Una vez más, scikit-Learn incluye una función para calcular esta métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizar el preprocesamiento en un *pipeline*\n",
    "\n",
    "En este caso, la curva ROC y su AUC indican que el modelo funciona mejor que una suposición aleatoria.\n",
    "\n",
    "En la práctica, es habitual preprocesar los datos para facilitar al algoritmo el ajuste de un modelo. Hay una gran variedad de transformaciones de preprocesamiento que se pueden realizar para preparar los datos para el modelado, pero nos limitaremos a algunas técnicas comunes:\n",
    "\n",
    "- Escalado de características numéricas para que estén en la misma escala. Esto evita que las características con valores grandes produzcan coeficientes que afecten desproporcionadamente a las predicciones.\n",
    "- Codificación de variables categóricas. Por ejemplo, utilizando una técnica de *one hot encoding* se pueden crear características binarias (verdadero/falso) individuales para cada posible valor de categoría.\n",
    "\n",
    "Para aplicar estas transformaciones de preprocesamiento, utilizaremos una función de Scikit-Learn denominada *pipelines*. Estas nos permiten definir un conjunto de pasos de preprocesamiento que terminan con un algoritmo. A continuación, puedse ajustar toda la tubería a los datos, de modo que el modelo encapsula todos los pasos de preprocesamiento, así como el algoritmo de regresión. \n",
    "\n",
    "Esto es útil, porque cuando queremos utilizar el modelo para predecir valores a partir de nuevos datos, necesitamos aplicar las mismas transformaciones (basadas en las mismas distribuciones estadísticas y codificaciones de categorías utilizadas con los datos de entrenamiento).\n",
    "\n",
    ">**Nota**: El término *pipeline* se utiliza mucho en el aprendizaje automático, ¡a menudo para significar cosas muy diferentes! En este contexto, lo estamos utilizando para referirnos a los objetos pipeline en Scikit-Learn, pero puede que lo veas utilizado en otros lugares para significar otra cosa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entrenamos el modelo\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Definir preprocesamiento para columnas numéricas (normalizarlas para que estén en la misma escala)\n",
    "numeric_features = [0,1,2,3,4,5,6]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Definir el preprocesamiento para características categóricas (codificar la columna Age)\n",
    "categorical_features = [7]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combinar los pasos del preprocesamiento\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Creación de un pipeline de preprocesamiento y formación\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('logregressor', LogisticRegression(C=1/reg, solver=\"liblinear\"))])\n",
    "\n",
    "\n",
    "# ajustar el pipeline para entrenar un modelo de regresión logística en el conjunto de entrenamiento\n",
    "model = pipeline.fit(X_train, (y_train))\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso *pipeline encapsulates*  hace los pasos de preprocesamiento y entrenamiento del modelo.\n",
    "\n",
    "Utilicemos el modelo entrenado por este proceso para predecir las clases de nuestro conjunto de pruebas y comparemos las métricas de rendimiento con el modelo básico que creamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtener predicciones a partir de datos de prueba\n",
    "predictions = model.predict(X_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Obtener métricas de evaluación\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print ('Confusion Matrix:\\n',cm, '\\n')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\",recall_score(y_test, predictions))\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "\n",
    "# calcular la curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# trazar la curva ROC\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Trazar la línea diagonal del 50%\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Representar gráficamente el FPR y el TPR obtenidos por nuestro modelo\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados parecen un poco mejores, así que está claro que el preprocesamiento de los datos ha marcado la diferencia.\n",
    "\n",
    "### Probar otro algoritmo\n",
    "Ahora vamos a probar un algoritmo diferente. Antes utilizamos un algoritmo de regresión logística, que es un algoritmo lineal. Hay muchos tipos de algoritmos de clasificación que podríamos probar, entre ellos\n",
    "\n",
    "- **Support Vector Machine algorithms**: Algoritmos que definen un *hiperplano* que separa las clases.\n",
    "- **Tree-based algorithms**:  Algoritmos que construyen un árbol de decisión para llegar a una predicción.\n",
    "- **Ensemble algorithms**: Algoritmos que combinan los resultados de varios algoritmos base para mejorar la generalizabilidad.\n",
    "\n",
    "En esta ocasión, utilizaremos los mismos pasos de preprocesamiento que antes, pero entrenaremos el modelo utilizando un algoritmo *ensemble* llamado *Random Forest* que combina las salidas de múltiples árboles de decisión aleatorios (para más detalles, consulte la documentación de Scikit-Learn) https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('logregressor', RandomForestClassifier(n_estimators=100))])\n",
    "\n",
    "# ajustar el pipeline para entrenar un modelo de bosque aleatorio en el conjunto de entrenamiento\n",
    "model = pipeline.fit(X_train, (y_train))\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los parámetros de rendimiento del nuevo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print ('Confusion Matrix:\\n',cm, '\\n')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\",recall_score(y_test, predictions))\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('\\nAUC: ' + str(auc))\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# trazar la curva ROC\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Trazar la línea diagonal del 50%\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Representar gráficamente el FPR y el TPR obtenidos por nuestro modelo\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizar el modelo para inferencias\n",
    "Ahora que tenemos un modelo entrenado razonablemente útil, podemos guardarlo para utilizarlo más adelante para predecir clases para nuevos datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar el modelo como archivo pickle\n",
    "filename = 'diabetes_model.pkl'\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando tenemos algunas observaciones nuevas cuya clase es desconocida, podemos cargar el modelo y utilizarlo para predecir los valores de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo desde el archivo\n",
    "model = joblib.load(filename)\n",
    "\n",
    "# predecir en una nueva muestra\n",
    "# El modelo acepta una matriz de matrices de características (para que pueda predecir las clases de múltiples pacientes en una sola llamada)\n",
    "# Crearemos un array con un único array de características, representando a un paciente\n",
    "X_new = np.array([[2,180,74,24,21,23.9091702,1.488172308,22]])\n",
    "print ('New sample: {}'.format(list(X_new[0])))\n",
    "\n",
    "# Obtener una predicción\n",
    "pred = model.predict(X_new)\n",
    "\n",
    "# El modelo devuelve una matriz de predicciones - una para cada conjunto de características presentadas\n",
    "# En nuestro caso, sólo hemos enviado un paciente, por lo que nuestra predicción es la primera de la matriz resultante.\n",
    "print('Predicted class is {}'.format(pred[0]))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-azureml_py38-py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
