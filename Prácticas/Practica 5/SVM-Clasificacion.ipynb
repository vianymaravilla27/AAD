{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19330d3",
   "metadata": {},
   "source": [
    "# Analítica Avanzada de Datos.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461e005",
   "metadata": {},
   "source": [
    "## Máquinas de Soporte Vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546066ca",
   "metadata": {},
   "source": [
    "Las máquinas de vectores soporte (SVM) son una clase particularmente potente y flexible de algoritmos supervisados tanto para la clasificación como para la regresión. En este notebook, exploraremos la intuición que hay detrás de las SVM y su uso en problemas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb973b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f35c74",
   "metadata": {},
   "source": [
    "En lugar de modelizar cada clase, nos limitaremos a encontrar una línea o curva (en dos dimensiones) o un múltiple (en múltiples dimensiones) que separe las clases entre sí.\n",
    "Como ejemplo de esto, consideremos el caso simple de una tarea de clasificación en la que las dos clases de puntos están bien separadas (véase la figura siguiente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2a47a",
   "metadata": {},
   "source": [
    "Un clasificador discriminativo lineal intentaría trazar una línea recta que separara los dos conjuntos de datos, y crear así un modelo para la clasificación. Para datos bidimensionales como los que se muestran aquí, ésta es una tarea que podríamos hacer a mano. Pero tenemos un problema: hay más de una línea divisoria posible que puede discriminar perfectamente entre las dos clases\n",
    "\n",
    "Podemos dibujar algunas de ellas de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaccd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6bc3b7",
   "metadata": {},
   "source": [
    "Se trata de tres separadores muy diferentes que, sin embargo, discriminan perfectamente entre estas muestras. Dependiendo del que elijamos, a un nuevo punto de datos (por ejemplo, el marcado con una \"X\" en este gráfico) se le asignará una etiqueta diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d9579",
   "metadata": {},
   "source": [
    "### Máquinas de vectores soporte: Maximización del margen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261311a",
   "metadata": {},
   "source": [
    "Las máquinas de soporte vectorial ofrecen una forma de mejorar esta situación. La intuición es la siguiente: en lugar de trazar simplemente una línea de anchura cero entre las clases, podemos dibujar alrededor de cada línea un margen de cierta anchura, hasta el punto más cercano. He aquí un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='lightgray', alpha=0.5)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca859ee5",
   "metadata": {},
   "source": [
    "La línea que maximiza este margen es la que elegiremos como modelo óptimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef578dd",
   "metadata": {},
   "source": [
    "### Ajuste de una máquina de soporte vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb44ef",
   "metadata": {},
   "source": [
    "Veamos el resultado de un ajuste real a estos datos: utilizaremos el clasificador de vectores de soporte (SVC) de Scikit-Learn para entrenar un modelo SVM en estos datos. Por el momento, vamos a utilizar un kernel lineal y establecer el parámetro C a un número muy grande:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca29ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbfea1",
   "metadata": {},
   "source": [
    "Para visualizar mejor lo que está sucediendo aquí, vamos a crear una función de conveniencia rápida que trazará los límites de decisión SVM para nosotros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0385dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, edgecolors='black',\n",
    "                   facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd3f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77159499",
   "metadata": {},
   "source": [
    "Se trata de la línea divisoria que maximiza el margen entre los dos conjuntos de puntos. Observa que algunos de los puntos de entrenamiento tocan el margen: están rodeados por un círculo. Estos puntos son los elementos fundamentales de este ajuste; se conocen como **vectores de soporte** y dan nombre al algoritmo. En *Scikit-Learn*, las identidades de estos puntos se almacenan en el atributo *support_vectors_* del clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2caffa",
   "metadata": {},
   "source": [
    "Una de las claves del éxito de este clasificador es que, para el ajuste, sólo importan las posiciones de los vectores de soporte; cualquier punto alejado del margen que esté en el lado correcto no modifica el ajuste. Técnicamente, esto se debe a que estos puntos no contribuyen a la función de pérdida utilizada para ajustar el modelo, por lo que su posición y número no importan mientras no crucen el margen.\n",
    "\n",
    "Podemos ver esto, por ejemplo, si trazamos el modelo aprendido a partir de los primeros 60 puntos y los primeros 120 puntos de este conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da8ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027920f",
   "metadata": {},
   "source": [
    "En el lado izquierdo, vemos el modelo y los vectores de apoyo para 60 puntos de entrenamiento. En el lado derecho, hemos duplicado el número de puntos de entrenamiento, pero el modelo no ha cambiado: los tres vectores de soporte del lado izquierdo son los mismos que los del lado derecho. **Esta insensibilidad al comportamiento exacto de los puntos distantes es uno de los puntos fuertes del modelo SVM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38df090",
   "metadata": {},
   "source": [
    "### Más allá de los límites lineales: SVM de núcleo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e2af9",
   "metadata": {},
   "source": [
    "SVM puede llegar a ser muy potente cuando se combina con los kernels. \n",
    "Para motivar la necesidad de núcleos, veamos algunos datos que no son linealmente separables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacaaea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9580e68",
   "metadata": {},
   "source": [
    "Está claro que ninguna discriminación lineal podrá separar estos datos. Pero podemos pensar en cómo podríamos proyectar los datos en una dimensión superior de forma que un separador lineal fuera suficiente. Por ejemplo, una proyección sencilla que podríamos utilizar sería calcular una función de base radial (RBF) centrada en el grupo central:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99995f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357a3ad6",
   "metadata": {},
   "source": [
    "Podemos visualizar esta dimensión adicional de los datos mediante un gráfico tridimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c10a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "ax.view_init(elev=20, azim=30)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea7c7e7",
   "metadata": {},
   "source": [
    "Podemos ver que con esta dimensión adicional, los datos se vuelven trivialmente separables linealmente, dibujando un plano de separación en, digamos, r=0,7\n",
    "\n",
    "En este caso, tuvimos que elegir y ajustar cuidadosamente nuestra proyección: si no hubiéramos centrado nuestra función de base radial en el lugar correcto, no habríamos obtenido unos resultados tan limpios y linealmente separables. En general, la necesidad de hacer una elección de este tipo es un problema: nos gustaría encontrar de algún modo automáticamente las mejores funciones de base para utilizar.\n",
    "\n",
    "Una estrategia para ello es *calcular una función base centrada en cada punto del conjunto de datos y dejar que el algoritmo SVM examine los resultados*. Este tipo de transformación de la función base se conoce como **transformación kernel**, ya que se basa en una relación de similitud (o kernel) entre cada par de puntos.\n",
    "\n",
    "Un problema potencial de esta estrategia -proyectar puntos en dimensiones- es que puede resultar muy intensiva desde el punto de vista computacional a medida que sea mayor. Sin embargo, gracias a un pequeño procedimiento conocido como el **truco del kernel**, se puede realizar un ajuste de los datos transformados por el kernel de forma implícita, es decir, sin tener que construir la representación dimensional completa del kernel. -de la proyección del kernel. Este truco de kernel está integrado en la SVM, y es una de las razones por las que el método es tan potente.\n",
    "\n",
    "En Scikit-Learn, podemos aplicar SVM kernelizado simplemente cambiando nuestro kernel lineal a un kernel RBF, utilizando el hiperparámetro del modelo de kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81834b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d9346",
   "metadata": {},
   "source": [
    "Utilicemos nuestra función previamente definida para visualizar el ajuste e identificar los vectores de soporte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fcab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef0eec3",
   "metadata": {},
   "source": [
    "Utilizando esta máquina de soporte vectorial kernelizada, aprendemos un límite de decisión no lineal adecuado. Esta estrategia de transformación de kernel se utiliza a menudo en el aprendizaje automático para convertir métodos lineales rápidos en métodos no lineales rápidos, especialmente para modelos en los que se puede utilizar el truco del kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0c457e",
   "metadata": {},
   "source": [
    "### Ajuste de la SVM: Suavizar los márgenes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037f076",
   "metadata": {},
   "source": [
    "Nuestra discusión hasta ahora se ha centrado en conjuntos de datos muy limpios, en los que existe un límite de decisión perfecto. Pero, ¿qué ocurre si sus datos se mezclan? Por ejemplo, puede tener datos como éstos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff0bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f741723d",
   "metadata": {},
   "source": [
    "Para manejar este caso, la implementación de SVM tiene un factor que \"suaviza\" el margen: es decir, permite que algunos de los puntos se cuelen en el margen si eso permite un mejor ajuste. La dureza del margen está controlada por un parámetro de ajuste, a menudo conocido como **C**. Para un **C** muy grande, el margen es duro, y los puntos no pueden estar en él. Con un **C** más pequeño, el margen es más suave y puede crecer hasta abarcar algunos puntos.\n",
    "\n",
    "El gráfico de la figura siguiente muestra cómo el cambio de **C** afecta al ajuste final a través de la suavización del margen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb98c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759840c6",
   "metadata": {},
   "source": [
    "El valor óptimo de C dependerá de su conjunto de datos, y deberá ajustar este parámetro utilizando la validación cruzada o un procedimiento similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b7f855",
   "metadata": {},
   "source": [
    "### Ejemplo: Reconocimiento facial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ecd4d3",
   "metadata": {},
   "source": [
    "Como ejemplo de máquinas de soporte vectorial, veamos el problema del reconocimiento facial. Utilizaremos el conjunto de datos *Labeled Faces in the Wild*, que consta de varios miles de fotos recopiladas de diversos personajes públicos. Scikit-Learn incorpora un recuperador para el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44908a",
   "metadata": {},
   "source": [
    "Vamos a trazar algunas de estas caras para ver con qué estamos trabajando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 5, figsize=(8, 6))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i], cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[],\n",
    "            xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aeaa60",
   "metadata": {},
   "source": [
    "Cada imagen contiene 62 × 47, es decir, unos 3.000 píxeles. Podríamos proceder simplemente utilizando el valor de cada píxel como característica, pero a menudo es más eficaz utilizar algún tipo de preprocesador para extraer características más significativas; aquí utilizaremos el análisis de componentes principales para extraer 150 componentes fundamentales para alimentar nuestro clasificador de máquina de vectores de soporte. Podemos hacerlo de la forma más sencilla empaquetando el preprocesador y el clasificador en un único canal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = PCA(n_components=150, whiten=True,\n",
    "          svd_solver='randomized', random_state=42)\n",
    "svc = SVC(kernel='rbf', class_weight='balanced')\n",
    "model = make_pipeline(pca, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de92dde",
   "metadata": {},
   "source": [
    "Para probar los resultados de nuestro clasificador, dividiremos los datos en un conjunto de entrenamiento y un conjunto de prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12700be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a74481",
   "metadata": {},
   "source": [
    "Por último, podemos utilizar la validación cruzada de búsqueda en cuadrícula para explorar combinaciones de parámetros. Aquí ajustaremos **C** (que controla la dureza del margen) y gamma (que controla el tamaño del núcleo de la función de base radial), y determinaremos el mejor modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'svc__C': [1, 5, 10, 50],\n",
    "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid)\n",
    "\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebb8bad",
   "metadata": {},
   "source": [
    "Los valores óptimos caen hacia el centro de nuestra cuadrícula; si cayeran en los bordes, querríamos ampliar la cuadrícula para asegurarnos de que hemos encontrado el verdadero óptimo.\n",
    "\n",
    "Ahora, con este modelo de validación cruzada, podemos predecir las etiquetas de los datos de prueba, que el modelo aún no ha visto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b43023",
   "metadata": {},
   "source": [
    "Veamos algunas de las imágenes de prueba junto con sus valores previstos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 6)\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
    "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
    "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3568e0a4",
   "metadata": {},
   "source": [
    "De esta pequeña muestra, nuestro estimador óptimo sólo etiquetó mal una cara (la cara de Bush en la fila inferior fue etiquetada erróneamente como Blair). Podemos hacernos una mejor idea del rendimiento de nuestro estimador utilizando el informe de clasificación, que enumera las estadísticas de recuperación etiqueta por etiqueta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7cb480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest, yfit,\n",
    "                            target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb667fa5",
   "metadata": {},
   "source": [
    "También podríamos mostrar la matriz de confusión entre estas clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67868769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "mat = confusion_matrix(ytest, yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
    "            cbar=False, cmap='Blues',\n",
    "            xticklabels=faces.target_names,\n",
    "            yticklabels=faces.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1fc27",
   "metadata": {},
   "source": [
    "Esto nos ayuda a hacernos una idea de qué etiquetas son susceptibles de ser confundidas por el estimador.\n",
    "\n",
    "Para una tarea de reconocimiento facial en el mundo real, en la que las fotos no vienen previamente recortadas en bonitas cuadrículas, la única diferencia en el esquema de clasificación facial es la selección de características: sería necesario utilizar un algoritmo más sofisticado para encontrar las caras y extraer características que sean independientes de la pixelación. Para este tipo de aplicación, una buena opción es utilizar OpenCV, que, entre otras cosas, incluye implementaciones preentrenadas de las herramientas de extracción de características más avanzadas para imágenes en general y rostros en particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1c306",
   "metadata": {},
   "source": [
    "Mayor información: https://scikit-learn.org/stable/modules/svm.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
