{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analítica Avanzada de Datos.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales en PyTorch\n",
    "\n",
    "El aprendizaje automático clásico se basa en el uso de estadísticas para determinar las relaciones entre características y etiquetas, y puede ser muy eficaz para modelos predictivos. Sin embargo, el crecimiento masivo de la disponibilidad de datos, unido a los avances en la tecnología necesarios para procesarlos, ha propiciado la aparición de nuevas técnicas de aprendizaje automático que imitan la forma en que el cerebro procesa la información en una estructura denominada red neuronal artificial\n",
    "\n",
    "PyTorch es un marco para crear modelos de aprendizaje automático, incuidas las redes neuronales profundas (DNNs). En este notebook, utilizaremos PyTorch para crear una red neuronal simple que clasifique a los pingüinos en especies basándose en la longitud y profundidad de su culmen (pico), la longitud de sus aletas y su masa corporal\n",
    "\n",
    "> **Cita**: El dataset de pingüinos que utilizaremos es un subconjunto de datos recogidos y puestos a disposición de la [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php)y [Palmer Station, Antarctica LTER](https://pal.lternet.edu/), miembro de [Long Term Ecological Research Network](https://lternet.edu/).\n",
    "\n",
    "## Explorando el Dataset\n",
    "\n",
    "Antes de empezar a utilizar PyTorch para crear el modelo, vamos a cargar los datos que necesitamos del dataset de pingüinos de las Islas Palmer, que contiene observaciones de tres especies diferentes de pingüinos.\n",
    "\n",
    "> **Nota**: En realidad, puedes resolver el problema de clasificación de pingüinos fácilmente utilizando técnicas clásicas de aprendizaje automático sin necesidad de un modelo de aprendizaje profundo; pero es un conjunto de datos útil y facil de entender con el que podemos ejemplificar los principios de las redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# cargar el conjunto de datos de entrenamiento (excluyendo las filas con valores nulos)\n",
    "penguins = pd.read_csv('data/penguins.csv').dropna()\n",
    "\n",
    "# Los modelos de deep learning funcionan mejor cuando las características están en escalas similares\n",
    "# En una solución real, implementaríamos alguna normalización personalizada para cada característica, pero para mantener las \n",
    "# cosas simples, sólo cambiaremos la escala de FlipperLength y BodyMass para que estén en una escala similar a las medidas de la factura\n",
    "penguins['FlipperLength'] = penguins['FlipperLength']/10\n",
    "penguins['BodyMass'] = penguins['BodyMass']/100\n",
    "\n",
    "# El conjunto de datos es demasiado pequeño para ser útil para el aprendizaje profundo\n",
    "# Así que vamos a sobremuestrearlo para aumentar su tamaño\n",
    "for i in range(1,3):\n",
    "    penguins = penguins.append(penguins)\n",
    "\n",
    "# Mostrar una muestra aleatoria de 10 observaciones\n",
    "sample = penguins.sample(10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna **Species** es la etiqueta que predicirá nuestro modelo. Cada valor de la etiqueta representa una clase de especies de pingüinos, codificada como 0, 1 ó 2. El siguiente código muestra las especies reales a las que corresponden estas etiquetas de clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguin_classes = ['Amelie', 'Gentoo', 'Chinstrap']\n",
    "print(sample.columns[0:5].values, 'SpeciesName')\n",
    "for index, row in penguins.sample(10).iterrows():\n",
    "    print('[',row[0], row[1], row[2],row[3], int(row[4]), ']',penguin_classes[int(row[-1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es habitual en un problema de aprendizaje supervisado, dividiremos el conjunto de datos en un conjunto de registros con los que entrenaremos el modelo y un conjunto más pequeño con el que validaremos el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = ['CulmenLength','CulmenDepth','FlipperLength','BodyMass']\n",
    "label = 'Species'\n",
    "   \n",
    "# Dividimos los datos 70%-30% en los conjuntos de entrenamiento y prueba\n",
    "x_train, x_test, y_train, y_test = train_test_split(penguins[features].values,\n",
    "                                                    penguins[label].values,\n",
    "                                                    test_size=0.30,\n",
    "                                                    random_state=0)\n",
    "\n",
    "print ('Training Set: %d, Test Set: %d \\n' % (len(x_train), len(x_test)))\n",
    "print(\"Sample of features and labels:\")\n",
    "\n",
    "# Daremos un vistazo a las primeras 25 características de entrenamiento y las etiquetas correspondientes\n",
    "for n in range(0,24):\n",
    "    print(x_train[n], y_train[n], '(' + penguin_classes[y_train[n]] + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las *características* son las medidas de cada observación de pingüino, y la *etiqueta* es un valor numérico que indica la especie de pingüino que representa la observación (Amelie, Papúa o Barbijo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar e importar las bibliotecas PyTorch\n",
    "\n",
    "Dado que planeamos utilizar PyTorch para crear nuestro clasificador de pingüinos, necesitaremos ejecutar las dos celdas siguientes para instalar e importar las bibliotecas PyTorch que pretendemos utilizar. La instalación específica de PyTorch depende de tu sistema operativo y de si tu computadora dispone de unidades de procesamiento gráfico (GPU) que puedan utilizarse para el procesamiento de alto rendimiento a través de cuda. Encontrará instrucciones detalladas en https://pytorch.org/get-started/locally/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as td\n",
    "\n",
    "# Set random seed for reproducability\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparar los datos para PyTorch\n",
    "\n",
    "PyTorch utiliza cargadores de datos para cargar los datos de entrenamiento y validación por lotes. Ya hemos cargado los datos en arrays numpy, pero necesitamos envolverlos en conjuntos de datos PyTorch (en los que los datos se convierten en objetos tensor PyTorch) y crear cargadores para leer lotes de esos conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un conjunto de datos y un cargador para los datos de entrenamiento y etiquetas\n",
    "train_x = torch.Tensor(x_train).float()\n",
    "train_y = torch.Tensor(y_train).long()\n",
    "train_ds = td.TensorDataset(train_x,train_y)\n",
    "train_loader = td.DataLoader(train_ds, batch_size=20,\n",
    "    shuffle=False, num_workers=1)\n",
    "\n",
    "# Crear un conjunto de datos y un cargador para los datos de prueba y las etiquetas\n",
    "test_x = torch.Tensor(x_test).float()\n",
    "test_y = torch.Tensor(y_test).long()\n",
    "test_ds = td.TensorDataset(test_x,test_y)\n",
    "test_loader = td.DataLoader(test_ds, batch_size=20,\n",
    "    shuffle=False, num_workers=1)\n",
    "print('Ready to load data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir una red neuronal\n",
    "\n",
    "Ahora estamos listos para definir nuestra red neuronal. En este caso, vamos a crear una red que consta de 3 capas totalmente conectadas:\n",
    "\n",
    "* Una capa de entrada que recibe un valor de entrada para cada característica (en este caso, las cuatro medidas de los pingüinos) y aplica una **función de activación ReLU**.\n",
    "\n",
    "* Una capa oculta que recibe diez entradas y aplica una función de activación ReLU.\n",
    "\n",
    "* Una capa de salida que utiliza una **función de activación Softmax** para generar una salida para cada especie de pingüino (que representan las probabilidades de clasificación para cada una de las tres posibles especies de pingüinos). Las funciones Softmax producen un vector con valores de probabilidad que suman 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de nodos en la capa oculta\n",
    "hl = 10\n",
    "\n",
    "# Definimos la red neuronal\n",
    "class PenguinNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PenguinNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(features), hl)\n",
    "        self.fc2 = nn.Linear(hl, hl)\n",
    "        self.fc3 = nn.Linear(hl, len(penguin_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.softmax(self.fc3(x),dim=1)\n",
    "        return x\n",
    "\n",
    "# Crear una instancia del modelo a partir de la red\n",
    "model = PenguinNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el modelo\n",
    "\n",
    "Para entrenar el modelo, necesitamos alimentar repetidamente los valores de entrenamiento a través de la red, utilizar una función de pérdida (loss cost) para calcular la pérdida, utilizar un optimizador para retropropagar (backpropagation) los ajustes de peso y valor de sesgo, y validar el modelo utilizando los datos de prueba que retuvimos.\n",
    "\n",
    "Para ello, vamos a crear una función para entrenar y optimizar el modelo, y una función para probar el modelo. A continuación, llamaremos a estas funciones de forma iterativa durante 50 épocas, el registro de la pérdida y las estadísticas de precisión para cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer):\n",
    "    # Poner el modelo en modo entrenamiento\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch, tensor in enumerate(data_loader):\n",
    "        data, target = tensor\n",
    "        #feedforward\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = loss_criteria(out, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # retropropagar\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #Perdida media por retorno\n",
    "    avg_loss = train_loss / (batch+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss\n",
    "           \n",
    "            \n",
    "def test(model, data_loader):\n",
    "    # Cambiar el modelo en modo evaluación (para no retropropagar)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for batch, tensor in enumerate(data_loader):\n",
    "            batch_count += 1\n",
    "            data, target = tensor\n",
    "            # Obtener las predicciones\n",
    "            out = model(data)\n",
    "\n",
    "            # calcular la pérdida\n",
    "            test_loss += loss_criteria(out, target).item()\n",
    "\n",
    "            # Calcular el accuracy\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "            \n",
    "    # Calcular la pérdida media y precisión total de esta época\n",
    "    avg_loss = test_loss/batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))\n",
    "    \n",
    "    # Pérdida media de retorno para la época\n",
    "    return avg_loss\n",
    "\n",
    "# Especificación de los criterios de pérdida (CrossEntropyLoss para la clasificación multiclase)\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Utilizar un optimizador \"Adam\" par ajustar los pesos\n",
    "# (consultar https://pytorch.org/docs/stable/optim.html#algorithms para detalles del algoritmo)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Seguiremos las métricas de cada época en estas matrices\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Entrenar más de 50 épocas\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    # imprimir el número de época\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    \n",
    "    # Introducir datos de entrenamiento en el modelo para optimizar las ponderaciones\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    \n",
    "    # Introducir los datos de prueba en el modelo para comprobar su rendimiento.\n",
    "    test_loss = test(model, test_loader)\n",
    "    \n",
    "    # Registrar las métricas de esta época\n",
    "    epoch_nums.append(epoch)\n",
    "    training_loss.append(train_loss)\n",
    "    validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mientras se ejecuta el proceso de formación, entendamos que esta ocurriendo:\n",
    "\n",
    "1. En cada época, el conjunto completo de datos de entrenamiento se transmite a través de la red. Hay cuatro características para cada observación y cuatro nodos correspondientes en la capa de entrada, por lo que las características de cada observación se transmiten como un vector de cuatro valores a esa capa. Sin embargo, por razones de eficacia, los vectores de características se agrupan en lotes, de modo que cada vez se introduce una matriz de múltiples vectores de características.\n",
    "\n",
    "2. La matriz de valores de características es procesada por una función que realiza una *suma ponderada* utilizando pesos y *valores de sesgo* inicializados. El resultado de esta función es procesado por la **función de activación** de la capa de entrada para limitar los valores transmitidos a los nodos de la capa siguiente.\n",
    "\n",
    "3. La *suma ponderada* y *las funciones de activación* se repiten en cada capa. Observe que las funciones operan sobre vectores y matrices en lugar de sobre valores escalares individuales. En otras palabras, el paso hacia delante es esencialmente una serie de funciones de álgebra lineal anidadas. Esta es la razón por la que los científicos de datos prefieren utilizar computadoras con unidades de procesamiento gráfico (GPU), ya que están optimizados para cálculos de matrices y vectores.\n",
    "\n",
    "4. En la última capa de la red, los vectores de salida contienen un valor de probabilidad para cada clase posible (en este caso, las clases 0, 1 y 2). Este vector es procesado por una *función de pérdida* para determinar la distancia entre los valores calculados por la red y los valores reales. Supongamos, por ejemplo, que la observación de un pingüino papúa (clase 1) es \\[0,3, 0,4, 0,3\\]. La predicción correcta debería ser \\[0,0, 1,0, 0,0\\], por lo que la varianza entre los valores predichos y reales (lo lejos que está cada valor predicho de lo que debería ser) es \\[0,3, 0,6, 0,3\\]. Esta varianza se agrega para cada lote y se mantiene como un agregado en ejecución para calcular el nivel general de error (*pérdida*) incurrido por los datos de entrenamiento para la época. \n",
    "\n",
    "5. Al final de cada época, los datos de validación se pasan por la red y también se calculan su *pérdida* y *precisión* (proporción de predicciones correctas basadas en el valor de probabilidad más alto del vector de salida). Es importante hacer esto porque nos permite comparar el rendimiento del modelo utilizando datos en los que no se ha entrenado, lo que nos ayuda a determinar si generalizará bien para los nuevos datos o si está *sobreajustado* a los datos de entrenamiento.\n",
    "\n",
    "6. Una vez que todos los datos han pasado a través de la red, la salida de la función de pérdida para los datos de *entrenamiento* (pero <u>no</u> los datos de *validación*) se pasa al optimizador. Los detalles precisos de cómo el optimizador procesa la pérdida varían dependiendo del algoritmo de optimización específico que se utilice; pero fundamentalmente se puede pensar en toda la red, desde la capa de entrada hasta la función de pérdida como una gran función anidada (*compuesta*). El optimizador aplica algo de **cálculo diferencial** para calcular *derivadas parciales* de la función con respecto a cada peso y valor de sesgo que se utilizó en la red. Es posible hacer esto eficientemente para una función anidada debido a algo llamado *regla de la cadena*, que permite determinar la derivada de una función compuesta a partir de las derivadas de su función interna y funciones externas. No es necesario preocuparse por los detalles matemáticos (el optimizador lo hace por ti), pero el resultado final es que las derivadas parciales nos indican la pendiente (o *gradiente*) de la función de pérdida con respecto a cada valor de peso y sesgo; en otras palabras, podemos determinar si aumentar o disminuir los valores de peso y sesgo para reducir la pérdida.\n",
    "\n",
    "7. Una vez determinada la dirección en la que deben ajustarse los pesos y los sesgos, el optimizador utiliza la *velocidad de aprendizaje* para determinar cuánto debe ajustarlos y, a continuación, trabaja hacia atrás a través de la red en un proceso denominado *backpropagation* para asignar nuevos valores a los pesos y sesgos de cada capa.\n",
    "\n",
    "8. Ahora la siguiente época repite todo el proceso de entrenamiento, validación y retropropagación comenzando con los pesos y sesgos revisados de la época anterior, lo que esperamos que resulte en un menor nivel de pérdida.\n",
    "\n",
    "9. El proceso continúa así durante 50 épocas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisar las pérdidas del entrenamiento y la validación\n",
    "\n",
    "Una vez completado el entrenamiento, podemos examinar las métricas de pérdida que registramos durante el entrenamiento y la validación del modelo. Realmente estamos buscando dos cosas:\n",
    "\n",
    "* La pérdida debe reducirse con cada epoch, mostrando que el modelo está aprendiendo los pesos y sesgos correctos para predecir las etiquetas correctas.\n",
    "* La pérdida de entrenamiento y la pérdida de validación deberían seguir una tendencia similar, mostrando que el modelo no se está sobreajustando a los datos de entrenamiento.\n",
    "\n",
    "Vamos a trazar las métricas de pérdida y ver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ver los pesos y sesgos aprendidos\n",
    "El modelo entrenado consiste en los pesos y sesgos finales que fueron determinados por el optimizador durante el entrenamiento. Basándonos en nuestro modelo de red, deberíamos esperar los siguientes valores para cada capa:\n",
    "\n",
    "* **Capa 1:** Hay cuatro valores de entrada que van a diez nodos de salida, por lo que debería haber 10 x 4 pesos y 10 valores de sesgo.\n",
    "* **Capa 2:** Hay diez valores de entrada que van a diez nodos de salida, por lo que debería haber 10 x 10 pesos y 10 valores de sesgo.\n",
    "* **Capa 3:** Hay diez valores de entrada que van a tres nodos de salida, por lo que debería haber 3 x 10 pesos y 3 valores de sesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\n\", model.state_dict()[param_tensor].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el rendimiento del modelo\n",
    "\n",
    "¿Es bueno el modelo? La precisión bruta de los datos de validación parece indicar que predice bastante bien, pero suele ser útil profundizar un poco más y comparar las predicciones de cada clase posible. Una forma habitual de visualizar el rendimiento de un modelo de clasificación es crear una matriz de confusión que muestre una tabla cruzada de predicciones correctas e incorrectas para cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch no tiene una métrica de matriz de confusión incorporada, así que usaremos SciKit-Learn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Poner el modelo en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Obtener predicciones para los datos de prueba\n",
    "x = torch.Tensor(x_test).float()\n",
    "_, predicted = torch.max(model(x).data, 1)\n",
    "\n",
    "# Trazar la matriz de confusión\n",
    "cm = confusion_matrix(y_test, predicted.numpy())\n",
    "plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(penguin_classes))\n",
    "plt.xticks(tick_marks, penguin_classes, rotation=45)\n",
    "plt.yticks(tick_marks, penguin_classes)\n",
    "plt.xlabel(\"Actual Species\")\n",
    "plt.ylabel(\"Predicted Species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de confusión debe mostrar una línea diagonal fuerte que indique que hay más predicciones correctas que incorrectas para cada clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el modelo entrenado\n",
    "\n",
    "Ahora que tenemos un modelo que creemos razonablemente preciso, podemos guardar sus pesos entrenados para utilizarlos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los pesos del modelo\n",
    "model_file = 'models/penguin_classifier.pt'\n",
    "torch.save(model.state_dict(), model_file)\n",
    "del model\n",
    "print('model saved as', model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizar el modelo entrenado\n",
    "\n",
    "Cuando tengamos una nueva observación de un pingüino, podemos utilizar el modelo para predecir la especie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Nuevas características del pingüino\n",
    "x_new = [[50.4,15.3,20,50]]\n",
    "print ('New sample: {}'.format(x_new))\n",
    "\n",
    "# Crear una nueva clase de modelo y cargar los pesos\n",
    "model = PenguinNet()\n",
    "model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "# Poner el modelo en modo evaluación\n",
    "model.eval()\n",
    "\n",
    "# Obtener una predicción para la nueva muestra de datos\n",
    "x = torch.Tensor(x_new).float()\n",
    "_, predicted = torch.max(model(x).data, 1)\n",
    "\n",
    "print('Prediction:',penguin_classes[predicted.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
